{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q Learning\n",
    "> by Khizr Ali Pardhan | Alik604\n",
    "\n",
    "[video](https://www.youtube.com/watch?v=RfNxXlO6BiA) | [Code](https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/DeepQLearning/torch_deep_q_model.py), [Code 2nd file](https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/DeepQLearning/main_torch_dqn_space_invaders.py) | [Code](https://github.com/BhavanJ/pytorch-spaceinvaders-/blob/master/Space_invaders_BJ.py) that I didnt use, but might be a good backup source\n",
    "\n",
    "### done\n",
    "* Steal some samplecode\n",
    "    - why reimplement the wheel? \n",
    "* [skim a paper](http://cs231n.stanford.edu/reports/2016/pdfs/106_Report.pdf) to feel smart | Recurrent Deep Q-Learning for PAC-MAN\n",
    "\n",
    "[source](https://medium.com/analytics-vidhya/introduction-to-double-deep-q-learning-ddqn-473833cf1a70) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In Double Deep Q Learning, the agent uses two neural networks to learn and predict what action to take at every step. \n",
    "\n",
    "One network, referred to as the Q network or the online network, is used to predict what to do when the agent encounters a new state. \n",
    "\n",
    "It takes in the state as input and outputs Q values for the possible actions that could be taken. \n",
    "\n",
    "the online network takes **a few frames**, *i think*, (observation) and **outputs a vector of N Q values**, one for the value of moving left in the current state, and one for the value of moving right in the current state.\n",
    "\n",
    "The agent will choose the action that has the higher corresponding Q value output by the online network. **like a argmax** \n",
    "\n",
    "Double DQNs handles the problem of the overestimation of Q-values.\n",
    "\n",
    "The solution is: when we compute the Q target, we tend to use 2 networks to decouple the action selected from the target Q value generation. \n",
    "\n",
    "We:\n",
    "* use our DQN network to select what is the best action required for the succeeding state (the action with the very best Q value).\n",
    "* use our target network to calculate the target Q value of taking that action at the next state.\n",
    "* Deep Q Network — selecting the best action a with maximum Q-value of next state.\n",
    "* Target Network — calculating the estimated Q-value with action a selected above.\n",
    "\n",
    "Therefore, Double Deep Q Network helps us reduce the overestimation of Q values and helps us train quicker and have more steady learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "I used a unfrozen pretrained `mobilenet_v2`. Its slow :(\n",
    "\n",
    "This change is reversable, you just have to swap-comment some lines\n",
    "\n",
    "Be sure to change `agent.EPSILON` to `agent.EPS_END`, the latter is by default 0.05\n",
    "\n",
    "To make Parallel\n",
    "https://www.reddit.com/r/MachineLearning/comments/8aimei/d_what_is_the_right_way_to_parallelize_rollouts/\n",
    "https://twitter.com/brandondamos/status/982699290492571654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "import time \n",
    "import pickle\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://raw.githubusercontent.com/philtabor/Youtube-Code-Repository/master/ReinforcementLearning/DeepQLearning/torch_deep_q_model.py  \n",
    "# video https://www.youtube.com/watch?v=RfNxXlO6BiA\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, ALPHA):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "#         #self.conv1 = nn.Conv2d(3, 32, 8, stride=4, padding=1)\n",
    "#         self.conv1 = nn.Conv2d(1, 64, 8, stride=4, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, 4, stride=2)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, 3)\n",
    "#         #self.fc1 = nn.Linear(128*23*16, 512)\n",
    "#         self.fc1 = nn.Linear(256*19*8, 512)\n",
    "#         self.fc2 = nn.Linear(512, 6)\n",
    "#         #self.optimizer = optim.SGD(self.parameters(), lr=self.ALPHA, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "        import torchvision.models as models\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        \n",
    "        # features is Sequence object, it is full of Conv and related layers. We don't touch that\n",
    "        # classifier is Sequence object with a nn.Dropout object at the first index\n",
    "        # and a nn.Linear object at the second/last index\n",
    "        num_ftrs = mobilenet.classifier[-1].in_features\n",
    "        # replace nn.Linear\n",
    "        mobilenet.classifier[-1] = nn.Linear(num_ftrs, 6)\n",
    "\n",
    "#         t_in = T.randn(64, 3, 256, 256)\n",
    "#         print(\"MobileNet v2:\", mobilenet.classifier, mobilenet(t_in).shape, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        self.mobilenet = mobilenet\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=ALPHA)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        print(f'Device: {self.device}')\n",
    "\n",
    "    def forward(self, observation):\n",
    "        observation = T.Tensor(observation).to(self.device)\n",
    "        #observation = observation.view(-1, 3, 210, 160) #.to(self.device)\n",
    "        observation = observation.view(-1, 3, 185, 95)\n",
    "        return self.mobilenet(observation)\n",
    "#         observation = observation.view(-1, 1, 185, 95)\n",
    "#         observation = F.relu(self.conv1(observation))\n",
    "#         observation = F.relu(self.conv2(observation))\n",
    "#         observation = F.relu(self.conv3(observation))\n",
    "#         #observation = observation.view(-1, 128*23*16).to(self.device)\n",
    "#         observation = observation.view(-1, 256*19*8)\n",
    "#         observation = F.relu(self.fc1(observation))\n",
    "#         actions = self.fc2(observation)\n",
    "#        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, gamma, epsilon, alpha,\n",
    "                 maxMemorySize, epsEnd=0.05,\n",
    "                 replace=10000, actionSpace=[0,1,2,3,4,5]):\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "        self.EPS_END = epsEnd\n",
    "        self.ALPHA = alpha\n",
    "        self.actionSpace = actionSpace\n",
    "        self.memSize = maxMemorySize\n",
    "        self.steps = 0\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = []\n",
    "        self.memCntr = 0\n",
    "        self.replace_target_cnt = replace\n",
    "        self.Q_eval = DeepQNetwork(alpha)\n",
    "        self.Q_next = DeepQNetwork(alpha)\n",
    "\n",
    "    def storeTransition(self, state, action, reward, state_):\n",
    "        if self.memCntr < self.memSize:\n",
    "            self.memory.append([state, action, reward, state_])\n",
    "        else:\n",
    "            self.memory[self.memCntr%self.memSize] = [state, action, reward, state_]\n",
    "        self.memCntr += 1\n",
    "\n",
    "    def chooseAction(self, observation):\n",
    "        rand = np.random.random()\n",
    "        actions = self.Q_eval.forward(observation)\n",
    "        if rand < 1 - self.EPSILON:\n",
    "            action = T.argmax(actions[1]).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.actionSpace)\n",
    "        self.steps += 1\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch_size):\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        if self.replace_target_cnt is not None and \\\n",
    "           self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
    "\n",
    "        if self.memCntr+batch_size < self.memSize:\n",
    "            memStart = int(np.random.choice(range(self.memCntr)))\n",
    "        else:\n",
    "            memStart = int(np.random.choice(range(self.memSize-batch_size-1)))\n",
    "        miniBatch=self.memory[memStart:memStart+batch_size]\n",
    "        memory = np.array(miniBatch)\n",
    "\n",
    "        # convert to list because memory is an array of numpy objects\n",
    "#         print(memory[:,0][:] == memory[:,0])\n",
    "\n",
    "        rewards = T.Tensor(list(memory[:,2])).to(self.Q_eval.device)\n",
    "        indices = np.arange(batch_size)\n",
    "        \n",
    "        # I can double train the below, if i send `memory` all at once\n",
    "        \n",
    "        Qnext = self.Q_next.forward(list(memory[:,3][:])).to(self.Q_eval.device)\n",
    "        maxA = T.argmax(Qnext, dim=1).to(self.Q_eval.device)      \n",
    "        \n",
    "        Qpred = self.Q_eval.forward(list(memory[:,0][:])).to(self.Q_eval.device)\n",
    "        Qtarget = Qpred.clone()        \n",
    "        Qtarget[indices,maxA] = rewards + self.GAMMA*T.max(Qnext[1])\n",
    "\n",
    "        #Qpred.requires_grad_()\n",
    "        loss = self.Q_eval.loss(Qtarget, Qpred).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        \n",
    "        if self.steps > 500:\n",
    "            if self.EPSILON - 1e-4 > self.EPS_END:\n",
    "                self.EPSILON -= 1e-4\n",
    "            else:\n",
    "                self.EPSILON = self.EPS_END        \n",
    "        \n",
    "        \n",
    "            \n",
    "#if self.iter_cntr % self.replace_target == 0:\n",
    "#    self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
    "                \n",
    "\n",
    "# This isn't my code. IDK why we dont optimize Q_next, however, I trust the author (youtube: machine learning with Phil). \n",
    "# This was because the two networks are different... IDK how to update the Q_next network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Box(0, 255, (210, 160, 3), uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym.make('CarRacing-v0')\n",
    "# env= gym.make('Pong-v0')\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space)\n",
    "# print(env.unwrapped.get_action_meanings())\n",
    "# help(env.unwrapped)\n",
    "\n",
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Device: cuda:0\n",
      "done initializing memory\n",
      "starting game  1 epsilon: 1.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kali\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 75.0\n",
      "starting game  2 epsilon: 1.5000\n",
      "score: 150.0\n",
      "starting game  3 epsilon: 1.5000\n",
      "score: 195.0\n",
      "starting game  4 epsilon: 1.4397\n",
      "score: 195.0\n",
      "starting game  5 epsilon: 1.3366\n",
      "score: 35.0\n",
      "starting game  6 epsilon: 1.2869\n",
      "score: 60.0\n",
      "Time taken: 4570.4892\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # i7 4790k & RTX 3070, 70mins \n",
    "time.sleep(3)\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "agent = Agent(gamma=0.95, epsilon=1.0*1.0, \n",
    "              alpha=0.003, maxMemorySize=5000,\n",
    "              replace=None)\n",
    "while agent.memCntr < agent.memSize:\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # 0 no action, 1 fire, 2 move right, 3 move left, 4 move right fire, 5 move left fire\n",
    "        action = env.action_space.sample()\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        if done and info['ale.lives'] == 0:\n",
    "            reward = -100\n",
    "#         agent.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
    "#                             np.mean(observation_[15:200,30:125], axis=2))\n",
    "        agent.storeTransition(observation[15:200,30:125], action, reward,\n",
    "                            observation_[15:200,30:125])\n",
    "        observation = observation_\n",
    "print('done initializing memory')\n",
    "\n",
    "best_score = 200 # yes, this is weird\n",
    "scores = []\n",
    "epsHistory = []\n",
    "numGames = 6\n",
    "batch_size=25 # 32\n",
    "# uncomment the line below to record every episode.\n",
    "# env = wrappers.Monitor(env, \"tmp/space-invaders-1\", video_callable=lambda episode_id: True, force=True)\n",
    "for i in range(numGames):\n",
    "    print('starting game ', i+1, 'epsilon: %.4f' % agent.EPSILON)\n",
    "    epsHistory.append(agent.EPSILON)\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    frames = [observation[15:200, 30:125]] #[np.sum(observation[15:200, 30:125], axis=2)]\n",
    "    score = 0\n",
    "    lastAction = 0\n",
    "    while not done:\n",
    "        if len(frames) == 3:\n",
    "            action = agent.chooseAction(frames)\n",
    "            frames = []\n",
    "        else:\n",
    "            action = lastAction\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "#         frames.append(np.sum(observation_[15:200, 30:125], axis=2))\n",
    "        frames.append(observation_[15:200, 30:125])\n",
    "\n",
    "        if done and info['ale.lives'] == 0:\n",
    "            reward = -100\n",
    "#         agent.storeTransition(np.mean(observation[15:200, 30:125], axis=2), action, reward,\n",
    "#                               np.mean(observation_[15:200, 30:125], axis=2))\n",
    "        agent.storeTransition(observation[15:200, 30:125], action, reward,\n",
    "                              observation_[15:200, 30:125])\n",
    "        observation = observation_\n",
    "        agent.learn(batch_size)\n",
    "        lastAction = action\n",
    "        #env.render()\n",
    "        \n",
    "    scores.append(score)\n",
    "    print('score:',score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        pickle.dump(agent, open(f'checkpoint_top_score{score}.p', \"wb\" ))   \n",
    "        \n",
    "x = [i+1 for i in range(numGames)]\n",
    "fileName = f'{str(numGames)} Games Gamma {str(agent.GAMMA)} Alpha {str(agent.ALPHA)} Memory {str(agent.memSize)}.png'\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time taken: {(end - start):.4f}')\n",
    "\n",
    "\n",
    "## Batch since 32. cart pole. eps_dec=5e-4*1.5\n",
    "# 31, 35 sec for CUDA, 80 steps.                             max 10-moving-average, 204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is:105.0\n"
     ]
    }
   ],
   "source": [
    "# utils.plotLearning(np.arange(len(scores)), scores[:], epsHistory, fileName)\n",
    "def bench(N=1):\n",
    "    for i in range(N):\n",
    "        done = False\n",
    "        score=0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "\n",
    "            done = False\n",
    "            observation = env.reset()\n",
    "            frames = [observation[15:200,30:125]]\n",
    "            score = 0\n",
    "            lastAction = 0\n",
    "            while not done:\n",
    "                if len(frames) == 3:\n",
    "                    action = agent.chooseAction(frames)\n",
    "                    frames = []\n",
    "                else:\n",
    "                    action = lastAction\n",
    "                observation_, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                frames.append(observation_[15:200,30:125])\n",
    "                if done and info['ale.lives'] == 0:\n",
    "                    reward = -100\n",
    "        #         agent.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
    "        #                               np.mean(observation_[15:200,30:125], axis=2))\n",
    "                observation = observation_\n",
    "        #         agent.learn(batch_size)\n",
    "                lastAction = action\n",
    "                env.render()\n",
    "        env.close()\n",
    "        print(f'Score is:{score}')\n",
    "bench(N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_more(n_games=50):\n",
    "    for i in range(n_games):\n",
    "        print('starting game ', i+1, 'epsilon: %.4f' % agent.EPSILON)\n",
    "        epsHistory.append(agent.EPSILON)\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        frames = [observation[15:200, 30:125]] #[np.sum(observation[15:200, 30:125], axis=2)]\n",
    "        score = 0\n",
    "        lastAction = 0\n",
    "        while not done:\n",
    "            if len(frames) == 3:\n",
    "                action = agent.chooseAction(frames)\n",
    "                frames = []\n",
    "            else:\n",
    "                action = lastAction\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            frames.append(observation_[15:200, 30:125])\n",
    "\n",
    "            if done and info['ale.lives'] == 0:\n",
    "                reward = -100\n",
    "\n",
    "            agent.storeTransition(observation[15:200, 30:125], action, reward,\n",
    "                                  observation_[15:200, 30:125])\n",
    "            observation = observation_\n",
    "            agent.learn(32)\n",
    "            lastAction = action\n",
    "\n",
    "        scores.append(score)\n",
    "        print('score:', score)\n",
    " \n",
    "    pickle.dump(agent, open(f'checkpoint_sapaceInvaders_mobilenet_top_score_{score}_epoch{len(scores)}.p', \"wb\" ))   \n",
    "            \n",
    "train_more(n_games=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(agent, open(f'checkpoint_top_score_{scores[-1]}.p', \"wb\" ))\n",
    "\n",
    "# agent = pickle.load(open(\"checkpoint_sapaceInvaders_mobilenet_top_score_130.0_epoch35.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.EPSILON = 0.0 # 0.2 #0.75\n",
    "agent.EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28064acbe88>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(agent, open(f'checkpoint_spaceInvader_mobilenet_foobar.p', \"wb\" ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
