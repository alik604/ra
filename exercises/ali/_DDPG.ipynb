{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "> Alik604\n",
    "\n",
    "is a model-free off-policy algorithm for learning continous actions. It combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network). It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces.\n",
    "\n",
    "### Source \n",
    "[code](https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/DDPG/pytorch/lunar-lander)\n",
    "\n",
    "## Results\n",
    "\n",
    "It didnt work for me... :'( \n",
    "\n",
    "trained with 0 changes, yes all 1000 epochs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='\\save'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        #self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        #self.fc1.bias.data.uniform_(-f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        #f2 = 0.002\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        #self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        #self.fc2.bias.data.uniform_(-f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        self.action_value = nn.Linear(self.n_actions, self.fc2_dims)\n",
    "        f3 = 0.003\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "        #self.q.weight.data.uniform_(-f3, f3)\n",
    "        #self.q.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "\n",
    "        action_value = F.relu(self.action_value(action))\n",
    "        state_action_value = F.relu(T.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='save'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        #self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        #self.fc1.bias.data.uniform_(-f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        #f2 = 0.002\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        #self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        #self.fc2.bias.data.uniform_(-f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        #f3 = 0.004\n",
    "        f3 = 0.003\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
    "        #self.mu.weight.data.uniform_(-f3, f3)\n",
    "        #self.mu.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = T.tanh(self.mu(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, input_dims, tau, env, gamma=0.99,\n",
    "                 n_actions=2, max_size=1000000, layer1_size=400,\n",
    "                 layer2_size=300, batch_size=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
    "                                  layer2_size, n_actions=n_actions,\n",
    "                                  name='Actor')\n",
    "        self.critic = CriticNetwork(beta, input_dims, layer1_size,\n",
    "                                    layer2_size, n_actions=n_actions,\n",
    "                                    name='Critic')\n",
    "\n",
    "        self.target_actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
    "                                         layer2_size, n_actions=n_actions,\n",
    "                                         name='TargetActor')\n",
    "        self.target_critic = CriticNetwork(beta, input_dims, layer1_size,\n",
    "                                           layer2_size, n_actions=n_actions,\n",
    "                                           name='TargetCritic')\n",
    "\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(self.noise(),\n",
    "                                 dtype=T.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                      self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.critic.device)\n",
    "        done = T.tensor(done).to(self.critic.device)\n",
    "        new_state = T.tensor(new_state, dtype=T.float).to(self.critic.device)\n",
    "        action = T.tensor(action, dtype=T.float).to(self.critic.device)\n",
    "        state = T.tensor(state, dtype=T.float).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_ = self.target_critic.forward(new_state, target_actions)\n",
    "        critic_value = self.critic.forward(state, action)\n",
    "\n",
    "        target = []\n",
    "        for j in range(self.batch_size):\n",
    "            target.append(reward[j] + self.gamma*critic_value_[j]*done[j])\n",
    "        target = T.tensor(target).to(self.critic.device)\n",
    "        target = target.view(self.batch_size, 1)\n",
    "\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.critic.eval()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                                      (1-tau)*target_critic_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                                      (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "        \"\"\"\n",
    "        #Verify that the copy assignment worked correctly\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(target_critic_params)\n",
    "        actor_state_dict = dict(target_actor_params)\n",
    "        print('\\nActor Networks', tau)\n",
    "        for name, param in self.actor.named_parameters():\n",
    "            print(name, T.equal(param, actor_state_dict[name]))\n",
    "        print('\\nCritic Networks', tau)\n",
    "        for name, param in self.critic.named_parameters():\n",
    "            print(name, T.equal(param, critic_state_dict[name]))\n",
    "        input()\n",
    "        \"\"\"\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "\n",
    "    def check_actor_params(self):\n",
    "        current_actor_params = self.actor.named_parameters()\n",
    "        current_actor_dict = dict(current_actor_params)\n",
    "        original_actor_dict = dict(self.original_actor.named_parameters())\n",
    "        original_critic_dict = dict(self.original_critic.named_parameters())\n",
    "        current_critic_params = self.critic.named_parameters()\n",
    "        current_critic_dict = dict(current_critic_params)\n",
    "        print('Checking Actor parameters')\n",
    "\n",
    "        for param in current_actor_dict:\n",
    "            print(param, T.equal(original_actor_dict[param], current_actor_dict[param]))\n",
    "        print('Checking critic parameters')\n",
    "        for param in current_critic_dict:\n",
    "            print(param, T.equal(original_critic_dict[param], current_critic_dict[param]))\n",
    "        input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearning(scores, filename, x=None, window=5):   \n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-window):(t+1)])\n",
    "    if x is None:\n",
    "        x = [i for i in range(N)]\n",
    "    plt.ylabel('Score')       \n",
    "    plt.xlabel('Game')                     \n",
    "    plt.plot(x, running_avg)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -163.05 trailing 50 games avg -163.046\n",
      "episode  1 score -441.17 trailing 50 games avg -302.105\n",
      "episode  2 score -758.22 trailing 50 games avg -454.143\n",
      "episode  3 score -421.18 trailing 50 games avg -445.902\n",
      "episode  4 score -672.01 trailing 50 games avg -491.124\n",
      "episode  5 score -198.60 trailing 50 games avg -442.369\n",
      "episode  6 score -169.26 trailing 50 games avg -403.354\n",
      "episode  7 score -385.02 trailing 50 games avg -401.063\n",
      "episode  8 score -418.44 trailing 50 games avg -402.993\n",
      "episode  9 score -536.23 trailing 50 games avg -416.317\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "agent = Agent(alpha=0.000025, beta=0.00025, input_dims=[8], tau=0.001, env=env,\n",
    "              batch_size=64,  layer1_size=400, layer2_size=300, n_actions=2)\n",
    "# agent.alpha = .00025\n",
    "# agent.beta  = .0025\n",
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []\n",
    "for i in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "\n",
    "    #if i % 25 == 0:\n",
    "    #    agent.save_models()\n",
    "\n",
    "    print('episode ', i, 'score %.2f' % score, 'trailing 50 games avg %.3f' % np.mean(score_history[-50:]))\n",
    "\n",
    "filename = 'LunarLander-alpha000025-beta00025-400-300.png'\n",
    "plotLearning(score_history, filename, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(agent, open(f'DDPG_lunar_con_{len(score_history)}.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -170.04 trailing 50 games avg -170.041\n",
      "episode  1 score -270.85 trailing 50 games avg -220.443\n"
     ]
    }
   ],
   "source": [
    "#agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []\n",
    "for i in range(2):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs)\n",
    "        new_state, reward, done, info = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        env.render()\n",
    "    env.close()\n",
    "    score_history.append(score)\n",
    "\n",
    "    #if i % 25 == 0:\n",
    "    #    agent.save_models()\n",
    "\n",
    "    print('episode ', i, 'score %.2f' % score, 'trailing 50 games avg %.3f' % np.mean(score_history[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
