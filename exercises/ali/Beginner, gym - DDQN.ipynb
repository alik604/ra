{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q Learning\n",
    "> by Khizr Ali Pardhan | Alik604\n",
    "\n",
    "\n",
    "### done\n",
    "* Steal some samplecode\n",
    "    - why reimplement the wheel? \n",
    "* [skim the paper](https://arxiv.org/pdf/1509.06461.pdf) to feel smart     \n",
    "    \n",
    "### todo \n",
    "\n",
    "* gym -> luner lander \n",
    "* gym addon -> robot\n",
    "    - https://github.com/nplan/gym-line-follower \n",
    "    - https://github.com/jr-robotics/robo-gym \n",
    "\n",
    "* try D4PG\n",
    "    - new notebook?\n",
    "\n",
    "more todo\n",
    "```\n",
    "https://www.youtube.com/watch?v=H9uCYnG3LlE\n",
    "https://www.youtube.com/watch?v=2vJtbAha3To\n",
    "https://www.youtube.com/watch?v=hlv79rcHws0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Q Learning\n",
    "[source](https://medium.com/analytics-vidhya/introduction-to-double-deep-q-learning-ddqn-473833cf1a70) \n",
    "\n",
    "In Double Deep Q Learning, the agent uses two neural networks to learn and predict what action to take at every step. \n",
    "\n",
    "One network, referred to as the Q network or the online network, is used to predict what to do when the agent encounters a new state. \n",
    "\n",
    "It takes in the state as input and outputs Q values for the possible actions that could be taken. \n",
    "\n",
    "the online network takes **in a vector of four values** (observation) and **outputs a vector of two Q values**, one for the value of moving left in the current state, and one for the value of moving right in the current state.\n",
    "\n",
    "The agent will choose the action that has the higher corresponding Q value output by the online network. **like a argmax** \n",
    "\n",
    "Double DQNs handles the problem of the overestimation of Q-values.\n",
    "\n",
    "The solution is: when we compute the Q target, we tend to use 2 networks to decouple the action selected from the target Q value generation. \n",
    "\n",
    "We:\n",
    "* use our DQN network to select what is the best action required for the succeeding state (the action with the very best Q value).\n",
    "* use our target network to calculate the target Q value of taking that action at the next state.\n",
    "* Deep Q Network — selecting the best action a with maximum Q-value of next state.\n",
    "* Target Network — calculating the estimated Q-value with action a selected above.\n",
    "\n",
    "Therefore, Double Deep Q Network helps us reduce the overestimation of Q values and helps us train quicker and have more steady learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "import time \n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://raw.githubusercontent.com/philtabor/Youtube-Code-Repository/master/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, \n",
    "            n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc2_5 = nn.Linear(self.fc2_dims, int(self.fc2_dims/2))\n",
    "        self.fc3 = nn.Linear(int(self.fc2_dims/2), self.n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        print(self.device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state.float()\n",
    "#         print(state.dtype)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc2_5(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "            max_mem_size=100000, eps_end=0.05, eps_dec=5e-4, ALIs_over_training=2):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 30\n",
    "        self.ALIs_over_training = ALIs_over_training\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=512, fc2_dims=256)\n",
    "        self.Q_next = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=512, fc2_dims=256) # 64 ,64, if not updating pramas\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        \n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        # replace=False means dont given duplicates. max_mem isnt mutated\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False) # todo decrease and force train on last 3 \n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "#         N = 2 if self.iter_cntr > self.batch_size else 1 # maybe use self.mem_cntr\n",
    "        for i in range(self.ALIs_over_training): # Ali over training \n",
    "            self.Q_eval.optimizer.zero_grad()\n",
    "            q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "            q_next = self.Q_eval.forward(new_state_batch)\n",
    "            q_next[terminal_batch] = 0.0\n",
    "\n",
    "            q_target = reward_batch + self.gamma*T.max(q_next, dim=1)[0]\n",
    "\n",
    "            loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "            loss.backward()\n",
    "            self.Q_eval.optimizer.step()\n",
    "\n",
    "            self.iter_cntr += 1\n",
    "            \n",
    "            if self.iter_cntr % self.replace_target == 0:\n",
    "                self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
    "                \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "        # This isn't my code. IDK why we dont optimize Q_next, however, I trust the author (youtube: machine learning with Phil). \n",
    "        # This was because the two networks are different... IDK how to update the Q_next network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1') # have batch size 32, and make the loop labled \"Ali over training\" run maybe 2 times \n",
    "# env = gym.make('LunarLander-v2') # score > 200 # have batch size 64, and make the loop labled \"Ali over training\" run maybe 2 to 5 times \n",
    "\n",
    "## these dont work this my code\n",
    "# env = gym.make(\"BipedalWalker-v3\")\n",
    "# env = gym.make('BipedalWalkerHardcore-v3')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env= gym.make('Pong-v0')\n",
    "\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space)\n",
    "# print(env.unwrapped.get_action_meanings())\n",
    "# help(env.unwrapped)\n",
    "\n",
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "episode  0 score 36.00 average score 36.00 epsilon 1.00\n",
      "episode  5 score 24.00 average score 23.83 epsilon 0.96\n",
      "episode  10 score 22.00 average score 26.70 epsilon 0.87\n",
      "episode  15 score 44.00 average score 31.70 epsilon 0.78\n",
      "episode  20 score 26.00 average score 40.90 epsilon 0.64\n",
      "episode  25 score 119.00 average score 59.80 epsilon 0.45\n",
      "episode  30 score 162.00 average score 93.10 epsilon 0.13\n",
      "episode  35 score 318.00 average score 169.50 epsilon 0.01\n",
      "episode  40 score 352.00 average score 281.20 epsilon 0.01\n",
      "episode  45 score 491.00 average score 371.20 epsilon 0.01\n",
      "episode  50 score 478.00 average score 410.90 epsilon 0.01\n",
      "episode  55 score 402.00 average score 388.30 epsilon 0.01\n",
      "episode  60 score 500.00 average score 372.80 epsilon 0.01\n",
      "episode  65 score 335.00 average score 319.70 epsilon 0.01\n",
      "episode  70 score 362.00 average score 289.80 epsilon 0.01\n",
      "episode  75 score 300.00 average score 340.70 epsilon 0.01\n",
      "episode  80 score 500.00 average score 344.40 epsilon 0.01\n",
      "episode  85 score 429.00 average score 330.50 epsilon 0.01\n",
      "episode  90 score 500.00 average score 346.70 epsilon 0.01\n",
      "episode  95 score 294.00 average score 331.10 epsilon 0.01\n",
      "episode  100 score 447.00 average score 336.50 epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=env.action_space.n, eps_end=0.01,\n",
    "              input_dims=[observation.shape[0]], lr=0.001, eps_dec=5e-4*1.1, ALIs_over_training=2) # changed from eps_dec=5e-4\n",
    "\n",
    "scores, eps_history = [], []\n",
    "n_games = 150\n",
    "\n",
    "start = time.time()\n",
    "time.sleep(3)\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    \n",
    "\n",
    "    if i % 5 == 0:\n",
    "        avg_score = np.mean(scores[-10:])\n",
    "        print('episode ', i, 'score %.2f' % score, 'average score %.2f' % avg_score, 'epsilon %.2f' % agent.epsilon)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time taken: {(end - start):.4f}')\n",
    "\n",
    "x = [i+1 for i in range(n_games)]\n",
    "filename = 'lunar_lander.png'\n",
    "\n",
    "## Batch since 32. cart pole. eps_dec=5e-4*1.5\n",
    "# 31, 35 sec for CUDA, 80 steps.                             max 10-moving-average, 204\n",
    "# 28, 30 sec for CPU, 80 steps.                              max 10-moving-average, 170\n",
    "# 16 sec for CPU reduced params, 80 steps.                   max 10-moving-average, 163\n",
    "# 44 sec for CUDA extra params, 80 steps.                    max 10-moving-average, 307 (or 197)\n",
    "# 55, 58 sec for CUDA extra params, bridge layer, 80 steps.  max 10-moving-average, 246, 261\n",
    "# 135 sec for CUDA extra params, bridge layer, DOUBLE TRAINING. 80 steps.  max 10-moving-average, 354 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plotLearning(np.arange(len(scores)), scores, eps_history, filename)\n",
    "\n",
    "done = False\n",
    "score=0\n",
    "observation = env.reset()\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = agent.choose_action(observation)\n",
    "    observation_, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "\n",
    "    agent.store_transition(observation, action, reward, observation_, done)\n",
    "    agent.learn()\n",
    "    observation = observation_\n",
    "env.close()\n",
    "print(f'Score is:{score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_more(n_games=50, avg_to_break = 350):\n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        avg_score = np.mean(scores[-10:])\n",
    "        print('episode ', i, 'score %.2f' % score, 'average score %.2f' % avg_score, 'epsilon %.2f' % agent.epsilon)\n",
    "        if avg_score > avg_to_break:\n",
    "            return # same as break\n",
    "    \n",
    "train_more(n_games=30, avg_to_break = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(agent, open(f'agent_CartPole-v1_{len(scores)}.p', \"wb\" ))\n",
    "\n",
    "# agent = pickle.load(open(\"agent_LunarLander-v2_400.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
